{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10488806,"sourceType":"datasetVersion","datasetId":6494155}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain_community\n!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:09:59.267420Z","iopub.execute_input":"2025-01-16T15:09:59.267730Z","iopub.status.idle":"2025-01-16T15:10:17.112876Z","shell.execute_reply.started":"2025-01-16T15:09:59.267705Z","shell.execute_reply":"2025-01-16T15:10:17.111511Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfplumber\n  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\nDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import spacy\nimport re\nimport pandas as pd\nfrom langchain_community.document_loaders import PDFPlumberLoader\nfrom spacy.matcher import Matcher","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:44:38.758308Z","iopub.execute_input":"2025-01-16T15:44:38.758668Z","iopub.status.idle":"2025-01-16T15:44:38.764064Z","shell.execute_reply.started":"2025-01-16T15:44:38.758643Z","shell.execute_reply":"2025-01-16T15:44:38.762105Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def extract_text_pdf(pdf_path):\n        loader = PDFPlumberLoader(pdf_path)\n        docs = loader.load()\n        text = ''\n        for doc in docs:\n            text += doc.page_content # Append extracted text\n        return text \nmatcher = Matcher(nlp.vocab)\ndef extract_name(nlp_text, matcher):\n    '''\n    Helper function to extract name from spacy nlp text\n\n    :param nlp_text: object of `spacy.tokens.doc.Doc`\n    :param matcher: object of `spacy.matcher.Matcher`\n    :return: string of full name\n    '''\n    pattern =  [{'POS': 'PROPN'}, {'POS': 'PROPN'}] \n    \"\"\"extrcted particular pattern variable\n        rather than imporing from a large data\n        \"\"\"\n    \n    matcher.add(\"NAME\", [pattern])# changed from \"matcher.add('NAME', None, *pattern)\"\n    \n    matches = matcher(nlp_text)\n    \n    for match_id, start, end in matches:\n        span = nlp_text[start:end]\n        return span.text\n\ndef extract_email(resume_text):\n    '''\n    Helper function to extract email id from text\n\n    :param text: plain text extracted from resume file\n    '''\n    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", resume_text)\n    if email:\n        try:\n            return email[0].split()[0].strip(';')\n        except IndexError:\n            return None\n\n\n\ndef extract_education_spacy(resume_text):\n    \"\"\"\n    Extracts the Education section from resume text using spaCy.\n    \n    :param resume_text: Plain resume text\n    :return: Extracted \"Education\" section as a string\n    \"\"\"\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    # Split the resume into lines\n    lines = resume_text.split(\"\\n\")\n\n    education_section = []\n    \n    # Extract lines after \"EDUCATION\" until the next section\n    for line in lines:\n        if education_section and re.search(r\"\\b(project|skills|work experience|certifications)\\b\", line, re.IGNORECASE):  \n            break \n    \n        if re.search(r\"\\b(education)\\b\", line, re.IGNORECASE) or education_section:  \n            education_section.append(line.strip())\n    \n    education_text = \"\\n\".join(education_section)\n\n    education_text = education_text.replace('\\\\n', ' ').replace('\\n', ' ').strip()\n\n    return education_text\n\n\nnoun_chunks = list(nlp_text.noun_chunks)\n\n\ndef extract_skills(nlp_text, noun_chunks):\n    '''\n    Helper function to extract skills from spacy nlp text\n\n    :param nlp_text: object of `spacy.tokens.doc.Doc`\n    :param noun_chunks: noun chunks extracted from nlp text\n    :return: list of skills extracted\n    '''\n    tokens = [token.text for token in nlp_text if not token.is_stop]\n    \n    # if noun_chunks is None:\n    #     noun_chunks = list(nlp_text.noun_chunks)\n\n    \n    url = \"https://raw.githubusercontent.com/OmkarPathak/ResumeParser/refs/heads/master/resume_parser/resume_parser/skills.csv\"\n    df = pd.read_csv(url, index_col=0)\n    data = df\n    \n    skills = list(data.columns.values)\n    \n    skillset = []\n    # check for one-grams\n    for token in tokens:\n        if token.lower() in skills:\n            skillset.append(token)\n    \n    # check for bi-grams and tri-grams\n    for token in noun_chunks:\n        token = token.text.lower().strip()\n        if token in skills:\n            skillset.append(token)\n    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n\n\ndef extract_experience_spacy(resume_text):\n    '''\n    Extracts the experience section from resume text using spaCy.\n    Also extracts job titles, company names, and dates.\n    \n    :param resume_text: Plain resume text\n    :return: Extracted \"Experience\" section as a string\n    '''\n    nlp = spacy.load(\"en_core_web_sm\")\n    nlp_text = nlp(text)\n\n    # Step 1: Extract the \"Experience\" section based on headers\n    lines = resume_text.split(\"\\n\")\n    experience_section = []\n    capture = False\n\n   # Step 1: Start capturing after \"WORK EXPERIENCE\" or similar headers\n    for line in lines:\n        if re.search(r\"\\b(experience|work experience|employment history)\\b\", line, re.IGNORECASE):  # Detect section start\n            capture = True\n        elif capture and re.search(r\"\\b(education|skills|projects)\\b\", line, re.IGNORECASE):  # Stop at next section\n            break\n        if capture:\n            experience_section.append(line.strip())\n\n    experience_text = \"\\n\".join(experience_section)\n    experience_text = experience_text.replace('\\\\n', ' ').replace('\\n', ' ').strip()\n\n    # Step 2: Extract structured entities using spaCy\n    experience_doc = nlp(experience_text)\n    extracted_info = {\n        \"Job Titles\": [],\n        \"Companies\": [],\n        \"Dates\": []\n    }\n    return experience_text\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:56:15.918405Z","iopub.execute_input":"2025-01-16T15:56:15.918757Z","iopub.status.idle":"2025-01-16T15:56:15.929858Z","shell.execute_reply.started":"2025-01-16T15:56:15.918737Z","shell.execute_reply":"2025-01-16T15:56:15.928801Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/steve-resume3/Steve Sun - Resume (2).pdf\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:51:32.078321Z","iopub.execute_input":"2025-01-16T15:51:32.078701Z","iopub.status.idle":"2025-01-16T15:51:32.083023Z","shell.execute_reply.started":"2025-01-16T15:51:32.078680Z","shell.execute_reply":"2025-01-16T15:51:32.081711Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"text = extract_text_pdf(pdf_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:51:34.638448Z","iopub.execute_input":"2025-01-16T15:51:34.638792Z","iopub.status.idle":"2025-01-16T15:51:34.760749Z","shell.execute_reply.started":"2025-01-16T15:51:34.638766Z","shell.execute_reply":"2025-01-16T15:51:34.759500Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"output = {\n    \"Name\": extract_name(nlp_text, matcher),\n    \"Email\": extract_email(text),\n    \"Skills\":extract_skills(nlp_text, noun_chunks),\n    \"Education\": extract_education_spacy(text),\n    \"Experience\": extract_experience_spacy(text),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:56:21.209082Z","iopub.execute_input":"2025-01-16T15:56:21.209388Z","iopub.status.idle":"2025-01-16T15:56:22.819056Z","shell.execute_reply.started":"2025-01-16T15:56:21.209367Z","shell.execute_reply":"2025-01-16T15:56:22.817766Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"print(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:56:22.820088Z","iopub.execute_input":"2025-01-16T15:56:22.821117Z","iopub.status.idle":"2025-01-16T15:56:22.826476Z","shell.execute_reply.started":"2025-01-16T15:56:22.821095Z","shell.execute_reply":"2025-01-16T15:56:22.824884Z"}},"outputs":[{"name":"stdout","text":"{'Name': 'Steve Sun', 'Email': 'stevesun1245@gmail.com', 'Skills': ['Kpis', 'Testing', 'Pdf', 'Cloud', 'Engagement', 'Modeling', 'Retention', 'Datasets', 'Python', 'Data analysis', 'Machine learning', 'Key performance indicators', 'Engineering', 'Analysis', 'Analytics', 'Predictive analytics', 'Pandas', 'Workflows', 'Process', 'Sql', 'Etl', 'Interactive', 'Ai', 'Parser', 'Communication'], 'Education': 'EDUCATION Skyline University Master of Science in Data Science - 3.81 GPA August 2018 – May 2020', 'Experience': 'WORK EXPERIENCE DataNova Insights Data Scientist March 2021 – Present • Developed and deployed machine learning models to optimize customer retention, increasing engagement by 15%. • Conducted exploratory data analysis (EDA) and statistical testing to derive actionable business insights. • Built scalable ETL pipelines to process large datasets using Python and SQL. • Collaborated with cross-functional teams to improve decision-making through predictive analytics. Neural Sphere Labs Data Scientist June 2019 – December 2023 • Designed and implemented deep learning models for image classification, improving accuracy by 20%. • Automated data preprocessing and feature engineering workflows using Python and Pandas. • Created interactive dashboards in Power BI to visualize key performance indicators (KPIs). • Partnered with engineers to deploy ML models into production, ensuring seamless integration.'}\n","output_type":"stream"}],"execution_count":62}]}