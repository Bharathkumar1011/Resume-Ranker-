{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1971405,"sourceType":"datasetVersion","datasetId":1177531}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:25:43.491288Z","iopub.execute_input":"2025-04-27T09:25:43.491534Z","iopub.status.idle":"2025-04-27T09:27:22.632854Z","shell.execute_reply.started":"2025-04-27T09:25:43.491510Z","shell.execute_reply":"2025-04-27T09:27:22.632117Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.2/437.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:24.179318Z","iopub.execute_input":"2025-04-27T09:33:24.179628Z","iopub.status.idle":"2025-04-27T09:33:24.453915Z","shell.execute_reply.started":"2025-04-27T09:33:24.179601Z","shell.execute_reply":"2025-04-27T09:33:24.453396Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport pandas as pd\nfrom typing import Optional, List, Tuple\nfrom datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:24.548772Z","iopub.execute_input":"2025-04-27T09:33:24.549399Z","iopub.status.idle":"2025-04-27T09:33:26.000004Z","shell.execute_reply.started":"2025-04-27T09:33:24.549375Z","shell.execute_reply":"2025-04-27T09:33:25.999261Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/resume-dataset/UpdatedResumeDataSet.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:30.848326Z","iopub.execute_input":"2025-04-27T09:33:30.848741Z","iopub.status.idle":"2025-04-27T09:33:30.948616Z","shell.execute_reply.started":"2025-04-27T09:33:30.848719Z","shell.execute_reply":"2025-04-27T09:33:30.947848Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Convert to HuggingFace Dataset\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:31.111919Z","iopub.execute_input":"2025-04-27T09:33:31.112595Z","iopub.status.idle":"2025-04-27T09:33:31.155370Z","shell.execute_reply.started":"2025-04-27T09:33:31.112570Z","shell.execute_reply":"2025-04-27T09:33:31.154806Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from langchain.docstore.document import Document as LangchainDocument\n\nRAW_KNOWLEDGE_BASE = [\n    LangchainDocument(page_content=doc[\"Resume\"], metadata={\"Category\": doc[\"Category\"]}) for doc in tqdm(dataset)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:32.324030Z","iopub.execute_input":"2025-04-27T09:33:32.324679Z","iopub.status.idle":"2025-04-27T09:33:32.718001Z","shell.execute_reply.started":"2025-04-27T09:33:32.324643Z","shell.execute_reply":"2025-04-27T09:33:32.717343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/962 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83525fe3fac14e2cab31f14c51f75ecf"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n# This list is taken from LangChain's MarkdownTextSplitter class\nMARKDOWN_SEPARATORS = [\n    \"\\n#{1,6} \",\n    \"```\\n\",\n    \"\\n\\\\*\\\\*\\\\*+\\n\",\n    \"\\n---+\\n\",\n    \"\\n___+\\n\",\n    \"\\n\\n\",\n    \"\\n\",\n    \" \",\n    \"\",\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n    chunk_overlap=100,  # The number of characters to overlap between chunks\n    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n    separators=MARKDOWN_SEPARATORS,\n)\n\ndocs_processed = []\nfor doc in RAW_KNOWLEDGE_BASE:\n    docs_processed += text_splitter.split_documents([doc])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:34.855907Z","iopub.execute_input":"2025-04-27T09:33:34.856631Z","iopub.status.idle":"2025-04-27T09:33:35.111327Z","shell.execute_reply.started":"2025-04-27T09:33:34.856605Z","shell.execute_reply":"2025-04-27T09:33:35.110773Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# docs_processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:38.811787Z","iopub.execute_input":"2025-04-27T09:33:38.812340Z","iopub.status.idle":"2025-04-27T09:33:38.817286Z","shell.execute_reply.started":"2025-04-27T09:33:38.812290Z","shell.execute_reply":"2025-04-27T09:33:38.816267Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\nprint(f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\nlengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:33:40.872251Z","iopub.execute_input":"2025-04-27T09:33:40.872846Z","iopub.status.idle":"2025-04-27T09:34:18.583718Z","shell.execute_reply.started":"2025-04-27T09:33:40.872822Z","shell.execute_reply":"2025-04-27T09:34:18.582815Z"}},"outputs":[{"name":"stderr","text":"2025-04-27 09:33:52.237509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745746432.425054      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745746432.478886      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa3308e5bcbe46389e288a80c61dc2c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16a456bdc694acd8b0208db68956e30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94abd30abe9743ada5f19fbf23425605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bad3382ff6a54029a66551e9293e6dfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ccfb9a8c67440e28417136ef04ea7c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"439ec021317c41709e4794961bdfec08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15af1f38581643aa921f43aa151ceeae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08a0630138dd45d79fcbdd0e1239d314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2930bae020c4cfb838723de9db56a4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c01b5321ad42dd8df30290c668f1f0"}},"metadata":{}},{"name":"stdout","text":"Model's maximum sequence length: 512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3893 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f9348bfb794b5a9b29644fed586208"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"\nEMBEDDING_MODEL_NAME = \"thenlper/gte-small\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:41.207601Z","iopub.execute_input":"2025-04-27T09:43:41.208307Z","iopub.status.idle":"2025-04-27T09:43:41.211601Z","shell.execute_reply.started":"2025-04-27T09:43:41.208283Z","shell.execute_reply":"2025-04-27T09:43:41.210915Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n)\n\nKNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:34:55.639620Z","iopub.execute_input":"2025-04-27T09:34:55.639813Z","iopub.status.idle":"2025-04-27T09:35:15.681305Z","shell.execute_reply.started":"2025-04-27T09:34:55.639797Z","shell.execute_reply":"2025-04-27T09:35:15.680710Z"}},"outputs":[{"name":"stderr","text":"2025-04-27 09:35:03.155011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745746503.176734     164 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745746503.183424     164 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c50976ef0fe45e8b1186b6e0e61eae4"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"KNOWLEDGE_VECTOR_DATABASE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:35:15.682476Z","iopub.execute_input":"2025-04-27T09:35:15.682705Z","iopub.status.idle":"2025-04-27T09:35:15.687168Z","shell.execute_reply.started":"2025-04-27T09:35:15.682688Z","shell.execute_reply":"2025-04-27T09:35:15.686625Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<langchain_community.vectorstores.faiss.FAISS at 0x7f0d33188390>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"retriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:33:17.598841Z","iopub.execute_input":"2025-04-27T10:33:17.599101Z","iopub.status.idle":"2025-04-27T10:33:17.662210Z","shell.execute_reply.started":"2025-04-27T10:33:17.599079Z","shell.execute_reply":"2025-04-27T10:33:17.661348Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/787850665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNOWLEDGE_VECTOR_DATABASE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'KNOWLEDGE_VECTOR_DATABASE' is not defined"],"ename":"NameError","evalue":"name 'KNOWLEDGE_VECTOR_DATABASE' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Embed a user query in the same space\nuser_query = \"retrive top 5 doc from data science catogery\"\nquery_vector = embedding_model.embed_query(user_query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:35:21.063309Z","iopub.execute_input":"2025-04-27T09:35:21.063571Z","iopub.status.idle":"2025-04-27T09:35:29.765630Z","shell.execute_reply.started":"2025-04-27T09:35:21.063552Z","shell.execute_reply":"2025-04-27T09:35:29.764989Z"}},"outputs":[{"name":"stderr","text":"2025-04-27 09:35:25.973456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745746525.994874     184 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745746526.001482     184 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e01e9dc0b0b143b081f2a933fd9889dd"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:35:29.766828Z","iopub.execute_input":"2025-04-27T09:35:29.767073Z","iopub.status.idle":"2025-04-27T09:35:38.442013Z","shell.execute_reply.started":"2025-04-27T09:35:29.767052Z","shell.execute_reply":"2025-04-27T09:35:38.441187Z"}},"outputs":[{"name":"stderr","text":"2025-04-27 09:35:34.696058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745746534.717628     204 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745746534.724201     204 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f17a8d3642454c83b85f9981e206c9f7"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import pprint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:39:01.699933Z","iopub.execute_input":"2025-04-27T09:39:01.700234Z","iopub.status.idle":"2025-04-27T09:39:01.703739Z","shell.execute_reply.started":"2025-04-27T09:39:01.700197Z","shell.execute_reply":"2025-04-27T09:39:01.703139Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"pprint.pp(retrieved_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:39:03.686777Z","iopub.execute_input":"2025-04-27T09:39:03.687344Z","iopub.status.idle":"2025-04-27T09:39:03.691676Z","shell.execute_reply.started":"2025-04-27T09:39:03.687321Z","shell.execute_reply":"2025-04-27T09:39:03.691059Z"}},"outputs":[{"name":"stdout","text":"[Document(id='660577a3-3224-490f-9171-c7996c67a8e4', metadata={'Category': 'Data Science', 'start_index': 3225}, page_content='â\\x9e\\x94 Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.\\r\\nThis project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.\\r\\n\\r\\nQuantifiable Results:\\r\\nâ\\x88\\x92 Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.\\r\\nâ\\x88\\x92 Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.\\r\\nâ\\x88\\x92 Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.'),\n Document(id='662e22e6-c98a-4087-8ec2-e2eefb1f44e3', metadata={'Category': 'Data Science', 'start_index': 3225}, page_content='â\\x9e\\x94 Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.\\r\\nThis project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.\\r\\n\\r\\nQuantifiable Results:\\r\\nâ\\x88\\x92 Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.\\r\\nâ\\x88\\x92 Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.\\r\\nâ\\x88\\x92 Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.'),\n Document(id='54772030-f78d-4d43-ad3a-3b444de05310', metadata={'Category': 'Data Science', 'start_index': 3225}, page_content='â\\x9e\\x94 Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.\\r\\nThis project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.\\r\\n\\r\\nQuantifiable Results:\\r\\nâ\\x88\\x92 Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.\\r\\nâ\\x88\\x92 Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.\\r\\nâ\\x88\\x92 Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.'),\n Document(id='7149de0c-f9e8-450c-ac49-5dacf07ac6b1', metadata={'Category': 'Data Science', 'start_index': 3225}, page_content='â\\x9e\\x94 Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.\\r\\nThis project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.\\r\\n\\r\\nQuantifiable Results:\\r\\nâ\\x88\\x92 Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.\\r\\nâ\\x88\\x92 Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.\\r\\nâ\\x88\\x92 Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.'),\n Document(id='26ba07bd-1bef-451e-9648-cde43b881d92', metadata={'Category': 'Data Science', 'start_index': 0}, page_content='Expertise â\\x88\\x92 Data and Quantitative Analysis â\\x88\\x92 Decision Analytics â\\x88\\x92 Predictive Modeling â\\x88\\x92 Data-Driven Personalization â\\x88\\x92 KPI Dashboards â\\x88\\x92 Big Data Queries and Interpretation â\\x88\\x92 Data Mining and Visualization Tools â\\x88\\x92 Machine Learning Algorithms â\\x88\\x92 Business Intelligence (BI) â\\x88\\x92 Research, Reports and Forecasts Education Details \\r\\n PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business\\r\\n B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy\\r\\nData Scientist \\r\\n\\r\\nData Scientist with PR Canada\\r\\nSkill Details \\r\\nAlgorithms- Exprience - 6 months\\r\\nBI- Exprience - 6 months\\r\\nBusiness Intelligence- Exprience - 6 months\\r\\nMachine Learning- Exprience - 24 months\\r\\nVisualization- Exprience - 24 months\\r\\nspark- Exprience - 24 months\\r\\npython- Exprience - 36 months\\r\\ntableau- Exprience - 36 months\\r\\nData Analysis- Exprience - 24 monthsCompany Details \\r\\ncompany - Aegis school of Data Science & Business')]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Create FAISS index\nvector_db = FAISS.from_documents(KNOWLEDGE_VECTOR_DATABASE, EMBEDDING_MODEL_NAME)\n\n# Save the index locally (optional)\nvector_db.save_local(\"resume_faiss_index\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:44:01.420048Z","iopub.execute_input":"2025-04-27T09:44:01.420356Z","iopub.status.idle":"2025-04-27T09:44:01.444028Z","shell.execute_reply.started":"2025-04-27T09:44:01.420334Z","shell.execute_reply":"2025-04-27T09:44:01.443015Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/219725567.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create FAISS index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvector_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNOWLEDGE_VECTOR_DATABASE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_MODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save the index locally (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvector_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resume_faiss_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mVectorStore\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVectorStore\u001b[0m \u001b[0minitialized\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \"\"\"\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'FAISS' object is not iterable"],"ename":"TypeError","evalue":"'FAISS' object is not iterable","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nREADER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\nmodel = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n\nREADER_LLM = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    do_sample=True,\n    temperature=0.2,\n    repetition_penalty=1.1,\n    return_full_text=False,\n    max_new_tokens=500,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T02:21:22.610322Z","iopub.execute_input":"2025-04-27T02:21:22.611211Z","iopub.status.idle":"2025-04-27T02:22:42.470761Z","shell.execute_reply.started":"2025-04-27T02:21:22.611185Z","shell.execute_reply":"2025-04-27T02:22:42.470148Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c357b24a7a3f45fd865ac9ea6351ca84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979c52a108a44e8f8fc2ccf361474e5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd6c097eb524250bc7d40fa0a034331"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1ffe2a249f4cbeba1670195be00f10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89577e8ad889489c9f132f10d443671d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61fc95b64d5848a69a772e1a382b50ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a1d15d972f2492fa09a45daf3253083"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d90038ed6e0a46f289cea24011145742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"516701c236a44971be6fa2e864ad3032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf2b63335e641a8812efe1e932c2053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ddd8987b08466ab87953cf44c819ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ae71d5ab38434c9e98d90c773d778f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a1704024d64f6e804f43c62ab829e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a80fbdeaa1fe4e86b3a8895bbcc29c06"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28eb8fe627e94f68aa92434143a7a641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d005479bd28479fa3f8535045832026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e275a19487e4412d8f508bbb2302aa21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02092f970ad44f09bd4540e4672ead9f"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"READER_LLM(\"how many resumes are in Data Science Category?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T02:23:30.406410Z","iopub.execute_input":"2025-04-27T02:23:30.406714Z","iopub.status.idle":"2025-04-27T02:23:35.809003Z","shell.execute_reply.started":"2025-04-27T02:23:30.406693Z","shell.execute_reply":"2025-04-27T02:23:35.808361Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': '\\n\\nI am interested in learning more about the Data Science category on your platform. Can you provide me with some statistics regarding the number of resumes available in this category and any trends or patterns that have emerged from analyzing these resumes? Additionally, could you suggest any best practices for optimizing a resume to stand out in this competitive field?'}]"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"prompt_in_chat_format = [\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"\n1. Analyze all the resumes to assess its alignment with the provided job description.\n2. Stick strictly to the facts provided in the resumes and job description. Never make assumptions or invent information.\n3. Select only the top 20 resumes based on their alignment with the job description. Ensure these are ranked clearly by relevance (1st: Most Suitable, 2nd: Next Best, 3rd: Final Option).\n4. Evaluate key areas, including:\n    Education: Does the candidate meet the educational requirements?\n    Experience: Is their work experience relevant and sufficient for the role?\n    Skills: Are the technical, soft, and domain-specific skills present?\n5. For each resume, provide:\n    Strengths: Key qualifications that match the role.\n    Gaps: Areas where the candidate does not meet the criteria.\n6. Summarize why these top 20 resumes were chosen, focusing on their alignment with the job requirements.\"\"\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"Context:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\"\"\",\n    },\n]\nRAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n)\nprint(RAG_PROMPT_TEMPLATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T02:28:57.710005Z","iopub.execute_input":"2025-04-27T02:28:57.710629Z","iopub.status.idle":"2025-04-27T02:28:57.715822Z","shell.execute_reply.started":"2025-04-27T02:28:57.710607Z","shell.execute_reply":"2025-04-27T02:28:57.715003Z"}},"outputs":[{"name":"stdout","text":"<|system|>\n\n1. Analyze all the resumes to assess its alignment with the provided job description.\n2. Stick strictly to the facts provided in the resumes and job description. Never make assumptions or invent information.\n3. Select only the top 20 resumes based on their alignment with the job description. Ensure these are ranked clearly by relevance (1st: Most Suitable, 2nd: Next Best, 3rd: Final Option).\n4. Evaluate key areas, including:\n    Education: Does the candidate meet the educational requirements?\n    Experience: Is their work experience relevant and sufficient for the role?\n    Skills: Are the technical, soft, and domain-specific skills present?\n5. For each resume, provide:\n    Strengths: Key qualifications that match the role.\n    Gaps: Areas where the candidate does not meet the criteria.\n6. Summarize why these top 20 resumes were chosen, focusing on their alignment with the job requirements.</s>\n<|user|>\nContext:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}</s>\n<|assistant|>\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"context = \"\"\"The following is the job description: 'Job Title: Data Scientist\n\nJob Duties:\nDeveloping and deploying machine learning models to solve business problems. \nCleaning, processing, and analyzing large datasets to extract actionable insights.\nCollaborating with cross-functional teams (engineering, product, marketing) to implement\ndata-driven solutions. Designing and maintaining data pipelines for efficient\ndata collection and processing. Conducting statistical analysis and A/B testing to \nmeasure the impact of business decisions. Building predictive models and recommendation\nsystems to enhance user experience. Creating data visualizations and dashboards to \ncommunicate findings to stakeholders. Staying updated with the latest advancements in \nAI/ML and applying best practices. Automating data workflows to improve\nefficiency and scalability. Other ad-hoc data-related tasks as required.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T02:28:58.001272Z","iopub.execute_input":"2025-04-27T02:28:58.001780Z","iopub.status.idle":"2025-04-27T02:28:58.005479Z","shell.execute_reply.started":"2025-04-27T02:28:58.001756Z","shell.execute_reply":"2025-04-27T02:28:58.004789Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"final_prompt = RAG_PROMPT_TEMPLATE.format(question=\"Select and Retrive only the top 20 resumes based on their alignment with the job description?\", context=context)\n\n# Redact an answer\nanswer = READER_LLM(final_prompt)[0][\"generated_text\"]\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T02:28:58.243868Z","iopub.execute_input":"2025-04-27T02:28:58.244101Z","iopub.status.idle":"2025-04-27T02:29:23.601521Z","shell.execute_reply.started":"2025-04-27T02:28:58.244085Z","shell.execute_reply":"2025-04-27T02:29:23.600885Z"}},"outputs":[{"name":"stdout","text":"To select and retrieve the top 20 resumes based on their alignment with the job description, follow these steps:\n\n1. Analyze all the resumes to assess their alignment with the provided job description. Look for candidates who have experience in developing and deploying machine learning models, cleaning and processing large datasets, collaborating with cross-functional teams, designing and maintaining data pipelines, conducting statistical analysis and A/B testing, building predictive models and recommendation systems, creating data visualizations and dashboards, staying updated with the latest advancements in AI/ML, automating data workflows, and performing other ad-hoc data-related tasks as required.\n\n2. Stick strictly to the facts provided in the resumes and job description. Do not make assumptions or invent information.\n\n3. Rank the resumes clearly by relevance (1st: Most Suitable, 2nd: Next Best, 3rd: Final Option). This will help you easily identify the top 20 resumes based on their alignment with the job description.\n\n4. Evaluate key areas, including education, experience, and skills. Make sure the candidate meets the educational requirements and has relevant work experience. Check if they possess the necessary technical, soft, and domain-specific skills.\n\n5. For each resume, provide strengths and gaps. Highlight the key qualifications that match the role and any areas where the candidate falls short of the criteria.\n\n6. Summarize why these top 20 resumes were chosen, focusing on their alignment with the job requirements. Explain how these candidates stand out from the rest due to their expertise in the specific areas mentioned in the job description.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}