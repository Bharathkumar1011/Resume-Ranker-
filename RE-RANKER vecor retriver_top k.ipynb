{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages (run this cell if not already installed)\n!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl datasets pypdf langchain-community langchain-huggingface ragatouille","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:16:01.982763Z","iopub.execute_input":"2025-05-20T13:16:01.982936Z","iopub.status.idle":"2025-05-20T13:17:32.767836Z","shell.execute_reply.started":"2025-05-20T13:16:01.982915Z","shell.execute_reply":"2025-05-20T13:17:32.767063Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.7/313.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pandas as pd\nimport os\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:18:17.315898Z","iopub.execute_input":"2025-05-20T13:18:17.316192Z","iopub.status.idle":"2025-05-20T13:18:17.592590Z","shell.execute_reply.started":"2025-05-20T13:18:17.316166Z","shell.execute_reply":"2025-05-20T13:18:17.591809Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom typing import Optional, List, Tuple\nfrom datasets import Dataset\nfrom langchain.document_loaders import DirectoryLoader, PyPDFLoader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:18:17.593794Z","iopub.execute_input":"2025-05-20T13:18:17.594269Z","iopub.status.idle":"2025-05-20T13:18:19.055143Z","shell.execute_reply.started":"2025-05-20T13:18:17.594243Z","shell.execute_reply":"2025-05-20T13:18:19.054608Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# down mergrng data all pdf fils and metadata from csv files to docs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef load_resumes(data_dir: str, csv_path: str = None) -> list:\n    \"\"\"Load resumes from directory and optional CSV metadata.\"\"\"\n    # Load CSV if provided (single line with error handling)\n    df = pd.read_csv(csv_path) if csv_path and os.path.exists(csv_path) else None\n    \n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"Directory not found: {data_dir}\")\n    \n    documents = []\n    for category in os.listdir(data_dir):\n        category_path = os.path.join(data_dir, category)\n        if not os.path.isdir(category_path):\n            continue\n            \n        # Load PDFs (removed progress bar for speed)\n        loader = DirectoryLoader(category_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n        docs = loader.load()\n        \n        for doc in docs:\n            # Simplified metadata extraction\n            doc.metadata.update({\n                \"category\": category,\n                \"file_name\": os.path.basename(doc.metadata[\"source\"]),\n                \"id\": os.path.splitext(doc.metadata[\"source\"].split('/')[-1])[0]\n            })\n            \n            # Optional CSV merge (one-liner)\n            if df is not None:\n                if match := df[df[\"ID\"] == doc.metadata[\"id\"]].to_dict('records'):\n                    doc.metadata.update(match[0])\n        \n        documents.extend(docs)\n    print(documents[0])\n    return documents\n\n# Usage (2 lines)\nDATA_DIR = \"/kaggle/input/resume-dataset/data/data\"\ndocs = load_resumes(DATA_DIR, \"/kaggle/input/resume-dataset/Resume/Resume.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:18:20.805214Z","iopub.execute_input":"2025-05-20T13:18:20.806059Z","iopub.status.idle":"2025-05-20T13:26:42.352973Z","shell.execute_reply.started":"2025-05-20T13:18:20.806030Z","shell.execute_reply":"2025-05-20T13:26:42.352333Z"}},"outputs":[{"name":"stdout","text":"page_content='PRE-PRESS GRAPHIC DESIGNER\nSummary\nCreative, hardworking designer seeking a full-time desktop job, educated as a graphic artist, past experience in business world as a desktop\npublisher laying out designs for printed mail and advertisements, in local government designing new websites with graphics for different agencies\nwithin the system, and later for the same government printing and reproduction center creating documents to be printed off a press or copiers.\nSkills\nAdobe InDesign, Photoshop, Illustrator, and Acrobat Professional\nStrongly familiar with Microsoft Word, Excel, PowerPoint, and Publisher / also QuarkXPress\nBasic knowledge of web development with Adobe Dreamweaver, HTML, WordPress\nAble to perform graphic design and administrative functions\nAble to work as a team player and independently\nExperienced using phone, fax, email, copiers and printers\nProvides excellent customer service (in-person, by phone, email, or interoffice mail)\nPrioritizes and calmly handles multiple projects and requests\nListens to directions, takes notes for later reference, follows procedures\nKnowledge of design setup on computer for jobs to be printed by outside vender or in-office copiers\nExperience\n01/2008\n \nto \nCurrent\nPre-Press Graphic Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nCreate new designs for variety of items like manuals, newsletters, and posters.\nUse templates for updated documents like envelopes, letterheads, and business cards.\nProof jobs for initial and final customer approval.\nManufactures a high-quality PDF file digitally for proofing, photocopying and offset printing.\nPerforms file backup and organizes system for easy recovery.\nMaintains and monitors supply inventory and orders items when needed.\nOperates photocopying equipment, includes sending approved documents to printer.\nAssists in the bindery department, using the folder and manual paper cutter for small jobs.\nAlso can use bindery equipment, like the fastback and GBC binding of spines.\nMounts and laminates to foam boards, manually trims to size.\nEnsures timely submission of files to production.\n04/2000\n \nto \n01/2008\nWeb Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nCreated new sites and made updates to current sites; created graphics to use on web pages; scanned documents and converted digital files\nfor links on sites; maintained updates and corrections on sites; answered email and phone call requests from departments about site changes;\nproofed pages with emails before sending live to internet.\n06/1998\n \nto \n02/2000\nDesktop Publisher\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nPerformed set-up and conversion of documents from Mac to PC then to UNIX systems; used QuarkXPress on Mac for the set-up of many\njobs; sent to network to be used by programmers for \"targeted\" direct mail printouts; trained new team staff members; helped with clean-up\nwhen company shut down.\n06/1997\n \nto \n03/2000\nGraphic Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nTemporary office jobs using Macintosh computers to design files to be printed for various companies like:.\nAlltel Publishing.\nCleveland School District.\nHKM Marketing Communications.\nNationwide Advertising.\nEducation and Training\nMay 1997\nBachelor of Fine Arts\n \nAlfred University\n \nï¼​ \nCity\n \n, \nState\nWork History\nCompany Name\nSkills\nadministrative functions, Acrobat, Adobe Dreamweaver, Photoshop, Advertising, backup, Basic, business cards, conversion, excellent customer\nservice, direct mail, email, fax, graphic design, graphics, HTML, Illustrator, Adobe InDesign, Mac, Macintosh computers, Marketing\nCommunications, Excel, mail, office, PowerPoint, Publisher, Microsoft Word, monitors, network, newsletters, takes notes, PDF, copiers, posters,\nprinter, printers, proofing, quality, QuarkXPress, supply inventory, team player, phone, UNIX, web development, web pages' metadata={'producer': 'Qt 4.8.7', 'creator': 'wkhtmltopdf 0.12.4', 'creationdate': '2021-08-08T15:32:08+05:30', 'title': '', 'source': '/kaggle/input/resume-dataset/data/data/DESIGNER/22506245.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'category': 'DESIGNER', 'file_name': '22506245.pdf', 'id': '22506245'}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(docs[0].page_content)  # First document in the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:33:58.132396Z","iopub.execute_input":"2025-05-20T13:33:58.133087Z","iopub.status.idle":"2025-05-20T13:33:58.137084Z","shell.execute_reply.started":"2025-05-20T13:33:58.133064Z","shell.execute_reply":"2025-05-20T13:33:58.136380Z"}},"outputs":[{"name":"stdout","text":"PRE-PRESS GRAPHIC DESIGNER\nSummary\nCreative, hardworking designer seeking a full-time desktop job, educated as a graphic artist, past experience in business world as a desktop\npublisher laying out designs for printed mail and advertisements, in local government designing new websites with graphics for different agencies\nwithin the system, and later for the same government printing and reproduction center creating documents to be printed off a press or copiers.\nSkills\nAdobe InDesign, Photoshop, Illustrator, and Acrobat Professional\nStrongly familiar with Microsoft Word, Excel, PowerPoint, and Publisher / also QuarkXPress\nBasic knowledge of web development with Adobe Dreamweaver, HTML, WordPress\nAble to perform graphic design and administrative functions\nAble to work as a team player and independently\nExperienced using phone, fax, email, copiers and printers\nProvides excellent customer service (in-person, by phone, email, or interoffice mail)\nPrioritizes and calmly handles multiple projects and requests\nListens to directions, takes notes for later reference, follows procedures\nKnowledge of design setup on computer for jobs to be printed by outside vender or in-office copiers\nExperience\n01/2008\n \nto \nCurrent\nPre-Press Graphic Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nCreate new designs for variety of items like manuals, newsletters, and posters.\nUse templates for updated documents like envelopes, letterheads, and business cards.\nProof jobs for initial and final customer approval.\nManufactures a high-quality PDF file digitally for proofing, photocopying and offset printing.\nPerforms file backup and organizes system for easy recovery.\nMaintains and monitors supply inventory and orders items when needed.\nOperates photocopying equipment, includes sending approved documents to printer.\nAssists in the bindery department, using the folder and manual paper cutter for small jobs.\nAlso can use bindery equipment, like the fastback and GBC binding of spines.\nMounts and laminates to foam boards, manually trims to size.\nEnsures timely submission of files to production.\n04/2000\n \nto \n01/2008\nWeb Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nCreated new sites and made updates to current sites; created graphics to use on web pages; scanned documents and converted digital files\nfor links on sites; maintained updates and corrections on sites; answered email and phone call requests from departments about site changes;\nproofed pages with emails before sending live to internet.\n06/1998\n \nto \n02/2000\nDesktop Publisher\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nPerformed set-up and conversion of documents from Mac to PC then to UNIX systems; used QuarkXPress on Mac for the set-up of many\njobs; sent to network to be used by programmers for \"targeted\" direct mail printouts; trained new team staff members; helped with clean-up\nwhen company shut down.\n06/1997\n \nto \n03/2000\nGraphic Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nTemporary office jobs using Macintosh computers to design files to be printed for various companies like:.\nAlltel Publishing.\nCleveland School District.\nHKM Marketing Communications.\nNationwide Advertising.\nEducation and Training\nMay 1997\nBachelor of Fine Arts\n \nAlfred University\n \nï¼​ \nCity\n \n, \nState\nWork History\nCompany Name\nSkills\nadministrative functions, Acrobat, Adobe Dreamweaver, Photoshop, Advertising, backup, Basic, business cards, conversion, excellent customer\nservice, direct mail, email, fax, graphic design, graphics, HTML, Illustrator, Adobe InDesign, Mac, Macintosh computers, Marketing\nCommunications, Excel, mail, office, PowerPoint, Publisher, Microsoft Word, monitors, network, newsletters, takes notes, PDF, copiers, posters,\nprinter, printers, proofing, quality, QuarkXPress, supply inventory, team player, phone, UNIX, web development, web pages\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# to create a raw k_b to feed into text spillter\nfrom langchain.docstore.document import Document as LangchainDocument\n\nRAW_KNOWLEDGE_BASE = [\n    LangchainDocument(page_content=doc.page_content, metadata={\"Category\": doc.metadata[\"category\"]}) for doc in docs\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:35:33.723807Z","iopub.execute_input":"2025-05-20T13:35:33.724092Z","iopub.status.idle":"2025-05-20T13:35:33.751212Z","shell.execute_reply.started":"2025-05-20T13:35:33.724072Z","shell.execute_reply":"2025-05-20T13:35:33.750113Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4881 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6cf34bd47124a80b831070741d9648f"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2588266499.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLangchainDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m RAW_KNOWLEDGE_BASE = [\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mLangchainDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n","\u001b[0;32m/tmp/ipykernel_31/2588266499.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m RAW_KNOWLEDGE_BASE = [\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mLangchainDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ]\n","\u001b[0;31mKeyError\u001b[0m: 'Category'"],"ename":"KeyError","evalue":"'Category'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# splitting raw d_b to feed into embeddoing model so we can create vector db\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n# This list is taken from LangChain's MarkdownTextSplitter class\nMARKDOWN_SEPARATORS = [\n    \"\\n#{1,6} \",\n    \"```\\n\",\n    \"\\n\\\\*\\\\*\\\\*+\\n\",\n    \"\\n---+\\n\",\n    \"\\n___+\\n\",\n    \"\\n\\n\",\n    \"\\n\",\n    \" \",\n    \"\",\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n    chunk_overlap=100,  # The number of characters to overlap between chunks\n    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n    separators=MARKDOWN_SEPARATORS,\n)\n\ndocs_processed = [] # this splitts the charectters into chuks and store\nfor doc in RAW_KNOWLEDGE_BASE:\n    docs_processed += text_splitter.split_documents([doc])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:36:05.800579Z","iopub.execute_input":"2025-05-20T13:36:05.801216Z","iopub.status.idle":"2025-05-20T13:36:06.661886Z","shell.execute_reply.started":"2025-05-20T13:36:05.801191Z","shell.execute_reply":"2025-05-20T13:36:06.661110Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# choosng andloadng emeddng model\nfrom sentence_transformers import SentenceTransformer\n\n# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\nprint(f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")\n\nfrom transformers import AutoTokenizer# model toknze the splited data\n\ntokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\") # here chuks get tokenized for vectors storage\nlengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:36:09.578578Z","iopub.execute_input":"2025-05-20T13:36:09.578887Z","iopub.status.idle":"2025-05-20T13:36:51.746768Z","shell.execute_reply.started":"2025-05-20T13:36:09.578864Z","shell.execute_reply":"2025-05-20T13:36:51.745969Z"}},"outputs":[{"name":"stderr","text":"2025-05-20 13:36:20.754483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747748180.943395      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747748180.995532      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e9043cdccf47bb97cbddcb0cf17751"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e42055d1b92468aa9685750ed8a3384"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca49aec2c7b2445cb7de874bcc33355c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40d3769e9a4e4dd98ac7362cd669627e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d6473a5e52542468162c8291eb65a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"087b68ce943b4a47bfd0120318190784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18fd718f2b14edabf9a66652ad93be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f3bd958c9c4e04b6b6fef7d60fd21b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00654d67ae09499e928d2a0302bfe916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"559e369b06ea4b9f978e34bf1b2a2829"}},"metadata":{}},{"name":"stdout","text":"Model's maximum sequence length: 512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18499 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3216cebaf4cf4bc58daa56c3c51b9c1b"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:37:30.715678Z","iopub.execute_input":"2025-05-20T13:37:30.716392Z","iopub.status.idle":"2025-05-20T13:37:30.719704Z","shell.execute_reply.started":"2025-05-20T13:37:30.716367Z","shell.execute_reply":"2025-05-20T13:37:30.718968Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from langchain.vectorstores import FAISS # stores vector databse\nfrom langchain_community.embeddings import HuggingFaceEmbeddings # fuction chain to perform embeddngs\nfrom langchain_community.vectorstores.utils import DistanceStrategy # for retriver to identify smlar docs\n\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n)\n\nKNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)\n# here we hve xomplete vector knoelgde base to perform query to retrive docs or perform llm to\n#generte ansrs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:37:30.950950Z","iopub.execute_input":"2025-05-20T13:37:30.951133Z","iopub.status.idle":"2025-05-20T13:38:25.953937Z","shell.execute_reply.started":"2025-05-20T13:37:30.951119Z","shell.execute_reply":"2025-05-20T13:38:25.953359Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2775243909.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedding_model = HuggingFaceEmbeddings(\n2025-05-20 13:37:40.021693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747748260.043264     126 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747748260.049905     126 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd800d327984a399c48b1776d3c92a6"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"KNOWLEDGE_VECTOR_DATABASE # high dimension matrx ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:38:32.971965Z","iopub.execute_input":"2025-05-20T13:38:32.972503Z","iopub.status.idle":"2025-05-20T13:38:32.977067Z","shell.execute_reply.started":"2025-05-20T13:38:32.972479Z","shell.execute_reply":"2025-05-20T13:38:32.976549Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<langchain_community.vectorstores.faiss.FAISS at 0x7e5d8f6d9310>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# its a doc retriver for vector db\nretriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever(search_kwargs={\"k\": 30})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:38:34.519860Z","iopub.execute_input":"2025-05-20T13:38:34.520448Z","iopub.status.idle":"2025-05-20T13:38:34.524228Z","shell.execute_reply.started":"2025-05-20T13:38:34.520424Z","shell.execute_reply":"2025-05-20T13:38:34.523532Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"docs = retriever.invoke(\"\"\"\n  \"Machine Learning Engineer\" OR \"Data Scientist\" AND (\n    (\"Python\" AND (\"Scikit-learn\" OR \"TensorFlow\" OR \"PyTorch\"))  \n    (\"SQL\" AND (\"Spark\" OR \"Hadoop\" OR \"ETL\"))  \n    (\"AWS\" OR \"GCP\" OR \"Azure\" OR \"MLOps\")  \n    (\"Tableau\" OR \"Power BI\" OR \"data visualization\")  \n    (\"statistical analysis\" OR \"A/B testing\")  \n    (\"NLP\" OR \"LLM\" OR \"GenAI\" OR \"recommendation systems\")  \n  )  \n  NOT (\"intern\" OR \"student\")  \n  Years: \"3+ years\"  \n  Location: \"Remote\" OR \"Mumbai\" OR \"Bangalore\" OR \"Pune\"  \n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:38:38.501377Z","iopub.execute_input":"2025-05-20T13:38:38.502030Z","iopub.status.idle":"2025-05-20T13:38:47.088059Z","shell.execute_reply.started":"2025-05-20T13:38:38.502005Z","shell.execute_reply":"2025-05-20T13:38:47.087294Z"}},"outputs":[{"name":"stderr","text":"2025-05-20 13:38:43.334457: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747748323.356176     146 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747748323.362945     146 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f84d836014848018c228fe0c6cdd03e"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import pprint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:38:48.577017Z","iopub.execute_input":"2025-05-20T13:38:48.577287Z","iopub.status.idle":"2025-05-20T13:38:48.581123Z","shell.execute_reply.started":"2025-05-20T13:38:48.577268Z","shell.execute_reply":"2025-05-20T13:38:48.580129Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"pprint.pp(docs) # retrved 30 docs to lter perform rerank","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:38:48.908989Z","iopub.execute_input":"2025-05-20T13:38:48.909242Z","iopub.status.idle":"2025-05-20T13:38:48.915564Z","shell.execute_reply.started":"2025-05-20T13:38:48.909224Z","shell.execute_reply":"2025-05-20T13:38:48.914886Z"}},"outputs":[{"name":"stdout","text":"[Document(id='fcff8e39-33dc-4ec9-8ef6-31c1b9c77493', metadata={'Category': 'ENGINEERING', 'start_index': 917}, page_content='implement statistical / predictive models and cutting edge algorithms utilizing diverse sources of data to predict Diversified experience with\\nEngineering, Manufacturing, Retailing, Higher Education and IT /Data related companies Fascinated by learning cutting edge technologies, such as;\\nData Mining and Machine Learning Handled a team of 4 during my 3 years of Tech experience and managed more than 200 students and\\neffectively evaluated the performance of each student and utilize assessment methods to judge overall progress during my teaching experience.\\nQualifications\\nAbility to identify uncovered information from hidden data and convert to a story and communicate effectively through visualization\\nModelling using R, SAS, Python using Pandas\\nWeb scraping using Beautiful soup in Python\\nDatabases like Oracle and Microsoft SQL\\nExperience in ASP.NET 4.5, C#, and HTML,\\nStatistical software like JMP, SPSS, GIS\\nVisualization software like Tableau, ggplot (R)'),\n Document(id='d8a2e74b-4906-4cc2-96a6-8b22c996645d', metadata={'Category': 'BUSINESS-DEVELOPMENT', 'start_index': 0}, page_content=\"BUSINESS DEVELOPMENT INTERN\\nSummary\\nObtain a position in analytics or data science in which I can enable data-driven decision-making to help leaders solve problems.\\nHighlights\\nProficient using Statistical Analysis Software (SAS), R, SAS Data Miner, SQL, Relational Databases, and Microsoft Office programs.\\n*Experienced in statistical analyses, sampling techniques, research design, C-level presentations, and professional writing skills.\\nExperience\\nBusiness Development Intern\\n \\n05/2013\\n \\nto \\nCurrent\\n \\nCompany Name\\n \\nCity\\n \\n, \\nState\\nEnabled precision micro-targeting and bid optimization with Search Engine Marketing (SEM) at the zip-code level for AutoTrader's clients.\\nUsed SAS and R to produce an interactive choropleth Google map that tracks page views, Sales, and Market Share of page views per\\nclient, which identifies potential areas of opportunity for SEM marketing.\"),\n Document(id='39aaaa44-58d0-4577-bfae-b2304c85a22d', metadata={'Category': 'ENGINEERING', 'start_index': 0}, page_content='ENGINEERING INTERN\\nSkills\\nC++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters.\\nMachine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost\\nmodel that has 77.5% accuracy in the Kaggle Titanic challenge.)\\nComputational Fluid Dynamics and Discrete Element Method Codes\\nCFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS.\\nReservoir and Fracture Modeling Tools\\nCMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis.\\nExperimental and Statistical Methods\\nSEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\\nExperience\\nENGINEERING INTERN\\n \\n08/2016\\n \\nï¼\\u200b \\n12/2016\\n \\nCompany Name\\n \\nState\\nProject: Develop a cavings transport model for optimizing hole-cleaning operations.'),\n Document(id='2d12b30c-9f35-4420-b6fa-91721b9b3922', metadata={'Category': 'AUTOMOBILE', 'start_index': 1637}, page_content=\"2011\\n \\nTATA CONSULTANCY SERVICES\\n \\nï¼\\u200b \\nCity\\n \\n, \\nState\\n \\n, \\nINDIA\\nSEPTEMBER 2011 - DECEMBER 2011Â \\nThis course provides an overview that gives business and information technology professionals the confidence to dive right into their business\\nintelligence and data warehousing activities. Hands-On training provided on ETL tools Informatica/Datastage andÂ data warehousing environment\\nfor 90 days.\\nDATASTAGE TRAINING\\n \\n2012\\n \\nGREENS TECHNOLOGIES\\n \\nï¼\\u200b \\nCity\\n \\n, \\nState\\n \\n, \\nINDIA\\n \\nThis course is designed to introduce ETL developers to\\nDatastage Development, Data Warehousing and Data Modeling training's with real-world ETL process implementations.\\nActivities and Honors\\nDISTINGUISHED ACHIEVER AWARD- TATA CONSULTANCY SERVICES [FEB 2015]\"),\n Document(id='b7469210-b5c9-436c-8a0d-9ce600e96dbc', metadata={'Category': 'CONSULTANT', 'start_index': 0}, page_content='Tested these classifiers using test data and compared the prediction accuracies among five different classifiers. Achieved 95% accuracy.\\nSoftware Engineer\\n \\nCompany Name\\n \\n| \\nCity\\n \\n, \\nState\\n \\n| \\nOctober 2013\\n \\n- \\nJanuary 2014\\nDevelop code in java and document artifacts including unit test plans and ensure that the output is as per the specifications:\\nExecute tasks with both procedural and OOP development techniques.\\nBest practices for efficient and easier to maintain code.\\nBest practices for securing web applications.\\nInserting, querying and managing data stored in databases or files.\\nSkills\\nSQL Server/MySQL\\nData Analysis and Visualization\\nC#/.NET framework\\nPython\\nJavaScript\\nAzure DevOps\\nEducation\\nMaster of Science\\n \\nComputer Science\\n \\nUniversity of Illinois At Springfield\\n \\n, \\nCity\\n \\n, \\nState\\nCompleted coursework in Data Science, C# Programming and .NET and Data Mining.\\n3.95/4.0 GPA\\nMay 2016\\nBachelor of Engineering\\n \\nInformation Science\\n \\nP.A College of Engineering (VTU)\\n \\n, \\nCity'),\n Document(id='829b18f2-f17a-4805-8963-9e621152dcb1', metadata={'Category': 'AUTOMOBILE', 'start_index': 0}, page_content='DATA ANALYST\\nProfessional Summary\\nIndustrial and Systems Engineering graduate, certified Base SAS Programmer and a Lean Six Sigma Green Belt with strong background in\\nstatistics, mathematics and logical problem solving looking for a dynamic opportunity in data driven fields of analytics and statistical modeling.\\nCore Qualifications\\nData Science Tools: R, Base SAS, Python (Numpy, Pandas, Matplotlib, Scikit- learn), SPSS, Minitab, MATLAB, Apache Spark, SQL, MS\\nExcel, MS Visio, Tableau MySQL, Oracle Database, Microsoft Access Key Competencies: Data Extraction, Data Wrangling, Data Analysis,\\nData Visualization, Regression Analysis (Linear, Logistic and Multinomial), Time Series Analysis, Association Rule Mining, Monte Carlo\\nSimulation, Optimization, Random Forests\\nExperience\\n07/2016\\n \\nto \\nCurrent\\nData Analyst\\n \\nCompany Name\\n \\nï¼\\u200b \\nState\\n09/2015\\n \\nto \\n05/2016\\nStudent Manager\\n \\nCompany Name\\n \\nï¼\\u200b \\nState'),\n Document(id='b62385a6-1783-4253-b067-30a1e663d081', metadata={'Category': 'INFORMATION-TECHNOLOGY', 'start_index': 0}, page_content='INFORMATION TECHNOLOGY INTERN (TEST AUTOMATION ENGINEER)\\nSummary\\nOver 3 yearsÂ of experience serving as a key contributor across all software development life cycleÂ phases includingÂ analysis,\\narchitectural design, prototyping, development, and testing of applicationÂ using Java/J2EE technologies in various domains.\\nVery good understanding of Object Oriented Programming, Data Structure, Algorithms, Design Patterns and Distributed Systems.\\nExcellent working experience in backendÂ development using different Spring modules like Spring Core ContainerÂ Module, AOP, MVC,\\nSecurity, Data, Transaction Management etc.\\nExperienced in developing Microservices with Spring Boot, Spring REST, Spring Cloud, etc.\\nExtensive experience in developing Web interfaces using HTML5, CSS3, Bootstrap, SASS, LESS, JavaScript, jQuery, AngularJS,\\nReactJS and BackboneJS.\\nExperienced in working with SQL databases like MySQL, PostgreSQL, Oracle and have some knowledge of NoSQL databases like\\nMongoDB.'),\n Document(id='709d2d19-9b4a-4481-a420-e4bd307bd20e', metadata={'Category': 'BANKING', 'start_index': 1831}, page_content=\"applications.\\nExperience with Social Analytics (close-knit Networks & Decision Trees) and some experience on supervised learning, Architecture &\\nDesign patterns and anti-patterns.\\nEducation\\n2016\\n \\nMassachusetts Institute of Technology\\n \\nM.B.A\\n \\n: \\n2002\\n \\nUCLA Anderson School of Management\\n \\n- \\nCity\\n \\n, \\nState\\n \\nBachelor's\\n \\n: \\nengineering\\n \\n, \\n1993\\n \\nThapar University\\n \\nSummary\\n18+ yrs. of experience in Information Technology Management with a proven record as a servant leader for large distributed teams in diverse\\ntechnical environments. Proven record of managing budgets, LRPs, product & portfolio roadmaps, business cases, software architecture,\\ndevelopment and operations. Extensive track record of nurturing deep relationships within the company, vendors, strategic partners and standards\\nbodies to achieve business goals. Strengths\\nCloud native architectures to drive reliability, performance and cost optimizations - IaaS, DBaaS, PaaS, Containerized, SaaS and Serverless\"),\n Document(id='b8dedb7d-2869-4e02-9e7d-a5e77faad009', metadata={'Category': 'BANKING', 'start_index': 819}, page_content='Pilot Run and User Acceptance testing.\\nApplication training, Go Live, Project sign-off.\\nWork with end-users to define and execute test scenarios and ensure appropriate end user training.\\nTechnical Responsibilities: Provide detailed system requirement to client(Hardware/Software).\\nProduction Server Setup (Windows 2003 Server / RedHat Linux 2.5).\\nDatabase setup (Oracle 10g R2.\\nIBM DB2,MS SQL 2005).\\nStandard database restore, Master Data preparation.\\nApplication server installation and configuration(Jboss 5.1.0.GA & Tomcat6).\\nSoftware deployments(ear,war etc.).\\nMaintaining Versions and Deliverable.\\nEducation\\nBachelor of Engineering\\n \\n, \\nInformation Technology\\n \\n6 2008\\n \\nUniversity of Mumbai\\n \\nIntegrated Trading and Manufacturing (ITM,An ERP by Base\\nInformation) BI Tool \\n: BI Base (Business Intelligence tool by Base Information) Information Technology\\nPersonal Information\\nComprehensive problem solving abilities, excellent verbal\\nInterests\\nPassport, Visa Details'),\n Document(id='187f5875-e2a1-4df4-b7a2-dc2d8604896a', metadata={'Category': 'ENGINEERING', 'start_index': 0}, page_content='INDUSTRIAL ENGINEERING INTERN\\nSummary\\nSeeking for full time position where I can apply my technical knowledge & skills for continuous improvement. I have 26 months of experience in\\nLean Manufacturing, Production Planning and Supply Chain Management. Excellent computer and analytical skills.\\nHighlights\\nApplication Software: MS Office, Minitab, PowerPoint, Excel, Solid works, AutoCAD, Pro E\\nDatabase: MS Access, SAP\\nProgramming: Java, C, C++, Visual Basic, R, Python\\nAccomplishments\\nQuality Control: Implementing of PDCA cycle for improving image quality of smartphone camera, implementing DMAIC cycle for\\nimproving quality issues related to cosmetic damages (scratches, bent, dent, etc.) using control charts and z transformation.\\nMetrics & Measurements: Time study, Process Mapping, JIT, &Work Sampling of Activities.\\nIntro to Statistics: Data analysis using normal & exponential distribution, simple & multi linear Regression.'),\n Document(id='28fb3e39-b406-4624-9333-f98f2e4f99b2', metadata={'Category': 'INFORMATION-TECHNOLOGY', 'start_index': 0}, page_content='BUSINESS SYSTEMS ANALYST I\\nQualifications\\nTECHNICAL SKILLS: Business Applications: SAP Web Intelligence, Informatica Data Explorer, MS Visio, MS Project, Rational Rose,\\nBusiness Objects Languages: SQL, UML,C, C++ , Core Java , Perl Web Development: HTML, XML, PHP Operating Systems: Windows\\nXP/Vista/7, Linux Databases: Netezza, MS SQL Server 2005/2008, Oracle 9i/10g\\nAccomplishments\\nOrganized workshops for SQL Server 2005 during the technical fest at Mumbai University Active member of Student Council of Asian\\nStudents at University of Maryland. Interface with the client and multi-disciplinary teams within Merkle (Business Intelligence, Information\\nTechnology, Database and Data Warehouse Developers) to support the solution delivery process\\nWork closely with clients to understand their marketing goals, design their marketing databases, facilitate optimum segmentation and provide\\nplatforms and reports to measure their marketing ROI.'),\n Document(id='5d4fa730-08cc-49b5-ae81-8e5e83421913', metadata={'Category': 'INFORMATION-TECHNOLOGY', 'start_index': 1778}, page_content=\"Information Technology and AWS Admin Intern\\n \\n, \\n04/2019\\n \\nCompany Name\\n \\nâ€“ \\nCity\\n \\n, \\nState\\n \\nResearched and implemented a secure cloud infrastructure for migrating the financial data into AWS with seamless integration \\nfor company's\\nfinancial department.\\nAchieved an overall increment of 30% in efficiency by migrating the company's infrastructure to AWS cloud.\\nReduced the company's infrastructure cost by 10 times.\\nImplemented secure off-site backups through scheduled weekly SSH dumps to remote server.\\nConfigured and troubleshot switches, routers and firewalls using TCP/IP protocols.\\nNetwork Engineer Intern\\n \\n, \\n10/2015\\n \\nCompany Name\\n \\nBuilt LAN and WAN for small-scale business enterprises using HTTP, DHCP, DNS, OSPF, VLAN.\\nAWS Certified Solutions Architect- Associate\\n \\n, \\n10/2018\\n \\nCompany Name\\n \\nâ€“ \\nCity\\n \\n, \\nState\\n \\nID-J007G7C1MFE41RSQ) Aug 2019 Cisco Certified Network Associate - CCNA 200-125 (ID-CSCO13264710.\\n04/2019\\n \\nCompany Name\"),\n Document(id='6cfb14f3-cdc1-4f48-88b3-6064bce27fe3', metadata={'Category': 'ENGINEERING', 'start_index': 951}, page_content='Â (Dynamics of Aerospace Vehicles/Avionics, Finite Element Methods, Aerodynamics, Professional Engineering and Communication and\\nThesis using MATLAB programming )\\nCertifications and Credentials\\nScrum Fundamentals Certified\\nData Science Fundamentals Certified\\nOracle SQL Developer\\nAdvanced SQL: SQL Expert (under progress)\\nExperience\\nEngineering intern\\n \\nFeb 2017\\n \\nto \\nApr 2017\\n \\nCompany Name\\n \\nï¼\\u200b \\nCity\\n \\n, \\nState\\nAchievements.\\nCollaborated with a team of engineers of different fields with the goal of creating technical solutions for product concerns raised by\\ncustomers and produced test record documentation containing solutions for customers.\\nContacted customers directly to verbally convey test result and explain solution procedure along with the written documentation.\\nSuccessfully independently scoped business opportunities through technically analysing possible options and presented information to the\\ncompany Director.'),\n Document(id='bca69b94-142f-48be-9b6d-f1e9080e0fe7', metadata={'Category': 'DESIGNER', 'start_index': 2834}, page_content='Skills\\nASP.NET, Adobe Acrobat, Dream Weaver, API, ASP, basic, C, C language, C++, Clinical Research, Controller, CSS3, Client, Data\\nManagement, Data Base, Database, Database management, Digital Video, Editor, Embedded C, Embedded System, computer graphics, HTML,\\nHTML5, Logic, Access 2000, C#, Excel, Exchange, Office, power point, MS Word, works, NHS Trust, Oracle 9, ORACLE, ORACLE8.0,\\nPLSQL, Page, PLC, programming, quality, recording, research, risk analysis, risk assessment, scheduling, Simulation, SQL, System Design, Visio,\\nVisual basics, Visual Basic 6.0, Visual Basic6.0, Web designer, website, Web designing, Author\\nEducation\\nMasters\\n \\n: \\nComputer Application\\n \\n, \\nComputer Application\\nMCA\\n \\n: \\nCMR Institute of Management studies, University of Banglore, INDIA. P.G\\n \\n- \\nTitle :Green House Controller, Platform: Embedded C - CX15, Client: NeSt Cyber Campus, Trivandrum, Kerala, India . Including 6 months'),\n Document(id='6841c26f-2116-41d4-9f65-742a6c9c6a53', metadata={'Category': 'ENGINEERING', 'start_index': 0}, page_content='B.E\\n \\n: \\nElectronics and Communication Engineering\\n \\nNational Institute of Technology\\n \\nï¼\\u200b \\nCity\\n \\nIndia\\n \\nElectronics and Communication Engineering\\nSkills\\nAPI, ATL, audio, backup, broadcast, C++, CLI, Hardware, concept, content management, Content, client, clients, documentation, dynamic\\nHTML5, editing, XML, FTP, functional, drawing, HTTP, IDs, explorer, JavaScript, json, LAN, MB, access, C#, MFC, Win, Windows, 2000,\\nNAS, OOP, page, Proxy, rendering, SAN, Scrum, servers, specification, team management, threads, troubleshoot, Video, Video Editing, Visual\\nC++'),\n Document(id='d3fe97d0-dce5-4501-9bc4-7a9bec270b46', metadata={'Category': 'AUTOMOBILE', 'start_index': 0}, page_content='Highlights\\nProg. Languages: \\nC (5+ yrs), Python (3+ yrs), Java (3+ yrs), MATLAB (Simulink) (5+ yrs), R (2 yrs), Processing (2yrs), SQL(4+ yrs),\\nPLC(2 yrs)\\nDoc. Editing: \\nWord/PPT/Excel, Pages/Numbers/Keynote, LATEX\\nMechanical Design: \\nAutoCAD (6 yrs), Solidworks (5+ yrs)\\nMechanical Skills: \\nMakerBot 3D print, Laser cut, Mill, Drill, Lathe Machine.\\nStatistics Softwares: \\nSTATA, SPSS\\nDatabase Softwares: \\nSQL Server (4 yrs), Navicat (2 yrs)\\nOperating Systems: \\nWindows 7/10, OS X\\nExperience\\nCompany Name\\n \\nJune 2016\\n \\nto \\nCurrent\\n \\nR&D Product Development Engineer\\n \\nCity\\nDesign and build a tail-sitter VTOL(vertical take off and landing) UAV(unmanned aerial vehicle) which.\\ntakes off and lands vertically and travels horizontally.\\nMain duties include but not limit to aerodynamics.\\nmodeling, UAV control system design, mechanical manufacturing, simulation and tuning/experiments.\\nCompany Name\\n \\nMay 2015\\n \\nto \\nFebruary 2016\\n \\nResearch assistant\\n \\nCity'),\n Document(id='8c75adbf-78c5-48c7-a607-88ed3296759d', metadata={'Category': 'AUTOMOBILE', 'start_index': 0}, page_content='Education\\nData Entry - Six Month Course\\n \\n: \\n1995\\n \\nKARANATAKA INFOTEK (Regd\\n \\nï¼\\u200b \\nCity\\n \\n, \\nIndia\\n \\nPUC 2Nd year\\nDiploma\\n \\n: \\nComputer Applications\\n \\n, \\n2002\\n \\nF. B. INTERNATIONAL Computer School\\n \\nï¼\\u200b \\nState\\n \\n, \\nIndia\\n \\nComputer Applications\\nPersonal Information\\nName \\n: Anand Father Name \\n: \\nChikkegowdappa Date of Birth : 23.06.1975 Age \\n: 37 Years Marital Status : Married\\nInterests\\nANAND.C # 35,11TH cross road Kurubarahalli road, Muneshwara layout Laggere, Bangalore-560058\\nLanguages\\nEnglish, Hindi, Kannada, Tamil, Luganda (African Language spoken in Uganda)\\nSkills\\nAgency, consultation, Clients, Data Entry, Engineer, English, Hindi, lathe, Repairs, Sales, Supervision, television, wiring\\nAdditional Information\\nCURRICULAM \\nVITAE ANAND.C 35,11TH cross road Kurubarahalli road, Muneshwara layout Laggere, Bangalore-560058\\nPersonal Details : Name \\n: Anand Father Name \\n: \\nChikkegowdappa Date of Birth : 23.06.1975 Age \\n: 37 Years Marital Status : Married\\nPassport No'),\n Document(id='c3c082b1-8205-4bf4-bfc2-a7b142b288b0', metadata={'Category': 'ENGINEERING', 'start_index': 960}, page_content='B. S\\n \\n: \\nComputer Science\\n \\n, \\n1988\\n \\nUnion College\\n \\nï¼\\u200b \\nCity\\n \\n, \\nState\\n \\nComputer Science\\nSkills\\n3D, Agile, AJAX, approach, B2B, budget, C, C++, competitive, CSS, database, delivery, e-commerce, Expert Systems, funds, hiring, HTML,\\nPHP, image, inspection, Java, JavaScript, Marketing, MongoDB, enterprise, Network, Networks, Neural, Oracle, PL/SQL, PCI, Perl, Product\\nDevelopment, profit, prototyping, Public safety, Python, real-time 3, receiving 6, requirement, Sales, SOAP, SQL, strategic, upgrades, upgrade,\\nvaluation, VC'),\n Document(id='aa6c22db-993c-4bba-a3bf-5a526e450dfc', metadata={'Category': 'INFORMATION-TECHNOLOGY', 'start_index': 0}, page_content=\"INFORMATION TECHNOLOGY AND AWS ADMIN INTERN\\nExperience\\nInformation Technology and AWS Admin Intern\\n \\n, \\n04/2019\\n \\nCompany Name\\n \\nâ€“ \\nCity\\n \\n, \\nState\\n \\nResearched and implemented a secure cloud infrastructure for migrating the financial data into AWS with seamless integration \\nfor company's\\nfinancial department.\\nAchieved an overall increment of 30% in efficiency by migrating the company's infrastructure to AWS cloud.\\nReduced the company's infrastructure cost by 10 times.\\nImplemented secure off-site backups through scheduled weekly SSH dumps to remote server.\\nConfigured and troubleshot switches, routers and firewalls using TCP/IP protocols.\\nNetwork Engineer Intern\\n \\n, \\n10/2015\\n \\nCompany Name\\n \\nBuilt LAN and WAN for small-scale business enterprises using HTTP, DHCP, DNS, OSPF, VLAN.\\nAWS Certified Solutions Architect- Associate\\n \\n, \\n10/2018\\n \\nCompany Name\\n \\nâ€“ \\nCity\\n \\n, \\nState\\n \\nID-J007G7C1MFE41RSQ) Aug 2019 Cisco Certified Network Associate - CCNA 200-125 (ID-CSCO13264710.\\n04/2019\"),\n Document(id='f749fa5f-db72-4cb8-acc1-f88f5d262e53', metadata={'Category': 'INFORMATION-TECHNOLOGY', 'start_index': 1798}, page_content=\"levels - Drive and track closure on any RF components and modules NCs and issues impact to production readiness - Work with cross-\\nfunctional teams to execute completion of satellite system design to fulfill contract requirement - Generate cascade RF performance\\nprediction analyses (i.e.\\nNF, Gain, IP3, 1dB-Comp, spurious, etc) - Exercises independent judgment in developing methods, techniques, and evaluation criterion for\\nobtaining results - Monitor and measure manufacturing processes to reduce losses, decrease time span and improve quality.\\nSystem Data Analyst\\n \\nAugust 2011\\n \\nto \\nDecember 2013\\n \\nCompany Name\\nPrioritize and extract big data from Purdue University's SQL database and maintain its accuracy and completeness - Develop and\\nimplement data collection systems strategies that optimize statistical efficiency and data quality - Data manipulation language SQL\\ncommands and utilize statistical tools including Excel, SAS, and SPSS.\"),\n Document(id='48874231-f4fa-47fc-88ae-e6ab7df80583', metadata={'Category': 'BANKING', 'start_index': 0}, page_content='SOFTWARE ENGINEER\\nQualifications\\nC# 3.0, PL/SQL, JavaScript, HTML 4, CSS 2 \\nFramework: .NET 3.5 \\nDatabase: SQL Server 2008, Oracle 9i \\nOperating systems: Windows\\n98/XP, Windows server 2000/2005, UNIX \\nTools/Services: TOAD, HP Service Manager, WinSCP, PuTTY, PM Smart, vi text editor, Visual\\nSourceSafe and BusinessObjects XI 3.1 Universe Designer, Desktop Intelligence, Central \\nManagement Console \\nDomain: Banking\\nWork Experience\\nSoftware Engineer\\n \\nJuly 2010\\n \\nto \\nMarch 2014\\n \\nCompany Name\\n3 years and 9 months \\nof experience in the development, support and enhancement of web and \\nwindows applications and in the\\nimplementation of cost effective valueadds \\nDesigned and developed webbased tools in \\nASP .NET 3.5, \\nusing the programming and \\nC#\\nPL/SQL \\nprocedural languages namely \\nand \\nEnhanced EBusiness applications and managed technological issues by analyzing codes and\\nproviding customizable solutions using SQL and \\nBusiness Objects XI 3.1 tools'),\n Document(id='b06b43f6-217e-428f-a57a-74c35a7c9fae', metadata={'Category': 'AUTOMOBILE', 'start_index': 850}, page_content='Jan 2012\\n \\nto \\nFeb 2013\\n \\nCompany Name\\n \\nï¼\\u200b \\nCity\\n \\n, \\nState\\nDATA MANAGEMENT RETURN TO SERVICE\\nThe primary objective of the project is to maintain Extract Transform Load (ETL) portfolio of projects at enterprise level.In a production\\nsupport role, provided quick problem resolution to daily, weekly, and monthly processing cycles executing in Datastage/Informatica. Over\\nthis period, I have established an exemplary record of providing successful system support and delivering business value for mid-level to\\nlarge business intelligence applications. Â Also worked on Service Requests developing ETL Datastage jobs for small business\\nrequirements.\\nEducation and Training\\nBACHELOR OF TECHNOLOGY\\n \\n2011\\n \\nANNA UNIVERSITY\\n \\nï¼\\u200b \\nCity\\n \\n, \\nState\\n \\n, \\nINDIA\\nBUSINESS INTELLIGENCE TRAINING\\n \\n2011\\n \\nTATA CONSULTANCY SERVICES\\n \\nï¼\\u200b \\nCity\\n \\n, \\nState\\n \\n, \\nINDIA\\nSEPTEMBER 2011 - DECEMBER 2011Â'),\n Document(id='4e556a8c-3bfd-4f2a-9360-32eb4eca9eed', metadata={'Category': 'ENGINEERING', 'start_index': 2767}, page_content=\"dispatch), Industrial Power System Protection, Power System \\nAnalysis.\\nBachelor of Technology\\n \\n: \\nInstrumentation & Control Engineering\\n \\nNirma University\\n \\nMay 2012\\n \\nCity\\n \\n, \\nIndia\\n \\nInstrumentation & Control Engineering\\n3.5\\nAnalog & Digital systems, Control System Design, Power Electronics, Instrumentation System.\\nCertifications\\nCertificate in SPI Intools from Smart Brains Engineering Pvt. Ltd. \\nApril 2014 - May 2014 *Skills acquired in SPI Intools, AutoCAD.\\nProfessional Affiliations\\nIEEE \\nApril 2015 - Present\\nSkills\\nAPI, Apollo, AutoCAD, Automation, C++, C++ programming, com, Controller, Electronics, engineer, engineering projects, FAT, Drawing, http,\\nISA, LabVIEW, Logic, MATLAB, MS office, NEC, Optimization, philosophy, PLC programming, PLC, RESEARCH, SCADA, Siemens,\\nSiemens PLC, simulation, System Design, System \\nAnalysis, teaching, wiring\\nAdditional Information\\nAWARDS, ACHIEVEMENTS & PUBLICATION \\n*12th Annual Graduate Research Conference (GRC 2016, UH), 'Fuzzy controlled\"),\n Document(id='b6056274-93d6-4d87-83f6-a1fd58d75791', metadata={'Category': 'AGRICULTURE', 'start_index': 0}, page_content='The Frameworks were Flask, Pandas and the Language was Phyton.\\nMy responsibilities were data cleansing and data categorization.\\nFrom the data visualization Leadership was able to get insight in the growth, sales, costs and make decisions towards better performance.\\nPre-Assess Melanoma: The objective of the project was to create a system that could pre-assess malignant melanoma using Matlab\\nLanguage and image processing algorithms.\\nThe system was trained with the images of malignant and healthy images.\\nMy responsibilities included all development, testing and training.\\nThe User Interface was the responsibility of other team member.\\nThis system enabled the users to seek healthcare at earlier stages of melanoma.\\nEducation\\nIN\\n \\n: \\nExpected in \\n06/2021\\n \\nRowan University\\n \\n- \\nCity\\n \\n, \\nState\\n \\nBachelor of Science\\n \\n: \\nComputer Engineering\\n \\n, \\n06/2015\\n \\nSelcuk University\\n \\n- \\nCity\\n \\nMaster of Science\\n \\nState\\n \\nWork History\\nSoftware Developer\\n \\n, \\n12/2015\\n \\nto \\nCurrent\\n \\nCompany Name\\n \\nâ€“'),\n Document(id='9b55f217-3836-471f-9d70-6ff8af8a0930', metadata={'Category': 'CONSULTANT', 'start_index': 2759}, page_content='databases and updates.\\nAnalyzed code and corrected errors to optimize output.\\nResolved customer issues by establishing workarounds and solutions to debug and create defect fixes.\\nWrote user manuals and other documentation for roll-out in customer training sessions.\\nEstablished and maintained key relationships with business stakeholders to promote future opportunities.\\nManaged 2 junior developers by delivering consistent coaching and constructive feedback.\\nCollege Student\\n \\nCompany Name\\n \\n| \\nCity\\n \\n, \\nState\\n \\n| \\nJanuary 2015\\n \\n- \\nMay 2016\\nMost accomplished College Projects:\\nPredicting Breast Cancer:\\nCreate machine learning model to predict malignant tumors.\\nUsed Python - \"Random Forest Classifier\" to predict malignant tumors in breast tissue.\\nHere, main idea is building multiple models with different sample and different initial variables from train data set.\\nGoal is to determine what attributes provide the most information that can be used to predict malignancy.'),\n Document(id='33c29010-a3ed-41c3-a8d0-40fb11e11a89', metadata={'Category': 'BANKING', 'start_index': 0}, page_content=\"INTERNSHIP\\nProfessional Overview\\nexperiences collecting and analyzing data with statistical methods, familiar with R and SAS programing, great knowledge of experiment design,\\nsampling techniques and documents management. strong skills in communication, group-working and work-planning.\\nCore Qualifications\\nStrong knowledge of SAS, R and SSPS programming\\nExcellent research skills\\nMicrosoft Word, Excel, PowerPoint\\nExcellent quantitative skills\\nTeam leadership\\nOrganizational planning\\nLeadership/communication skills\\nCustomer-oriented\\nEducation\\nUniversity of Missouri\\n \\n2015\\n \\nMaster of Science\\n \\n: \\nStatistics\\n \\nCity\\n \\n, \\nState\\n \\n, \\nThe United States\\nPresident of Chinese students' and scholars' association\\n3.7 GPA\\nCoursework in practical statistical models, data analysis 1, 2, 3, statistical software and data analysis, introduction of probability theory, statistical\\ninference, time series analysis, survival analysis, experimental design, current development in statistics.\\nSAS advanced license\"),\n Document(id='b46a07e9-b96f-4f01-a866-e45418744d67', metadata={'Category': 'ENGINEERING', 'start_index': 0}, page_content='ENGINEERING AND QUALITY TECHNICIAN\\nCareer Overview\\nA highly experienced skilled graduate with Analytics degree with a very good experience in SAS, Web scraping, SQL, Predictive modelling and\\ndata visualization. Excellent ability in identifying data requirements for analysis, data cleaning, munging and model building; Ensures the organization\\nuses it effectively to reach profit and growth objectives. Comfortable with data handling, modeling, and coding, and have an appreciation of what\\nmakes sense from a business standpoint. More than six years of experience working as a researcher, data analyst, and environmental science and\\nTechnology Instructor. Experience in SQL, data warehousing, maintaining, securing and stabilizing data layers and testing to identify data and\\nproduct defects introduced in the system. Customer segmentation, product positioning and mapping and conjoint analysis Modelling: Design and'),\n Document(id='d9139a2d-2380-4e66-9dc0-aec5b3cbb70d', metadata={'Category': 'INFORMATION-TECHNOLOGY', 'start_index': 4532}, page_content='power Â· Data acquisition and signal processing using Matlab\\nSkills\\nbig data, C, C++, charts, Circuit design, hardware, Data acquisition, data analyst, data collection, data mining, databases, database, dBm, DTI,\\ndesign software, documentation, functional, GSM, innovation, Java, LabView, Team leader, Logic Analyzer, Mac, manufacturing processes,\\nMatlab, Excel, Microsoft office, Office, Microwave, Radar, NCs, Network, dB, packaging, pivot tables, Programming, project design, proposals,\\nPublication, Python, quality, requirement, research, SAS, self-starter, Spectrum analyzer, SPSS, SQL, SSL, statistics, surveys, system design,\\ntroubleshooting, validation'),\n Document(id='b86c6411-fcb8-4a8d-8400-e68cd77daccc', metadata={'Category': 'BANKING', 'start_index': 892}, page_content=\"Achievements from the project: from the model we can find that there is a up trend in the future about this stock and this is a appropriate time to\\nbuy this piece of stock.\\nExperience\\nCompany Name\\n \\nMay 2014\\n \\nto \\nAugust 2014\\n \\ninternship\\n \\nCity\\n \\n, \\nState\\nMain tasks of this job:\\n Â \\n1.Collect, enter and analyze stock prices: collect useful data and enter them into the database in a correct classification way, use statistical methods\\nto analyze them based on R programming.\\n2.Predict the future trend of different stocks and divide them into different category: use time series methods to predict the trend\\n3.Write summary report: use statistical methods to analyze data with R and write the performance evaluation report based on quantitative facts.\\n4.Customer service: answer the phone call and solve customers' problems and do the reception job at the front table. Also offer the instruction\\nabout stocks and how to choose them to new clients.\"),\n Document(id='c76061f8-2662-4030-a349-11292b495d29', metadata={'Category': 'AUTOMOBILE', 'start_index': 2677}, page_content='Education\\nAug 2016\\nMasters of Science\\n \\n: \\nIndustrial and Systems Engineering\\n \\nBinghamton University, State University of New York\\n \\nIndustrial and Systems Engineering\\n3.51/4.00\\nMay 2014\\nBachelors of Engineering\\n \\n: \\nMechanical Engineering\\n \\nOsmania University\\n \\nMechanical Engineering 3.33/4.00\\nSkills\\nApache, AutoCAD, charts, Credit, clients, Data Analysis, Data Visualization, Databases, Dec, decision-making, layout, leadership, MATLAB,\\nMicrosoft Access, MS Excel, Minitab, MySQL, Optimization, Oracle Database, Python, SAS, Simulation, Six Sigma, SPSS, SQL, Statistical\\nAnalysis, Tableau, Visio, website')]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!pip install flashrank # rerank lib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:53:00.107981Z","iopub.execute_input":"2025-05-20T13:53:00.108510Z","iopub.status.idle":"2025-05-20T13:53:05.088984Z","shell.execute_reply.started":"2025-05-20T13:53:00.108488Z","shell.execute_reply":"2025-05-20T13:53:05.088245Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting flashrank\n  Downloading FlashRank-0.2.10-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from flashrank) (0.21.0)\nCollecting onnxruntime (from flashrank)\n  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flashrank) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flashrank) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from flashrank) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (2.4.1)\nCollecting coloredlogs (from onnxruntime->flashrank)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (25.2.10)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (24.2)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (1.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (2025.1.31)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->flashrank) (0.30.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (2024.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (4.13.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->flashrank)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->flashrank) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->flashrank) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->flashrank) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->flashrank) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime->flashrank) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->flashrank) (2024.2.0)\nDownloading FlashRank-0.2.10-py3-none-any.whl (14 kB)\nDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, flashrank\nSuccessfully installed coloredlogs-15.0.1 flashrank-0.2.10 humanfriendly-10.0 onnxruntime-1.22.0\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.document_compressors import FlashrankRerank\nfrom flashrank import Ranker  # Make sure `pip install flashrank` is done\n\n# from langchain_openai import ChatOpenAI\n\n# we have to defne our orignl retrver here to perform rerank\nretriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever(search_kwargs={\"k\": 30})\n\n# Step 2: Initialize Flashrank with a reranker model (NOT gte-small)\nranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\")  # Default model\ncompressor = FlashrankRerank(model=ranker, top_n=5)   # Rerank to top 5\n\n# compressor = FlashrankRerank(top_n= 5)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor, base_retriever=retriever\n)\n\ncompressed_docs = compression_retriever.invoke(\n    \"\"\"\n  \"Machine Learning Engineer\" OR \"Data Scientist\" AND (\n    (\"Python\" AND (\"Scikit-learn\" OR \"TensorFlow\" OR \"PyTorch\"))  \n    (\"SQL\" AND (\"Spark\" OR \"Hadoop\" OR \"ETL\"))  \n    (\"AWS\" OR \"GCP\" OR \"Azure\" OR \"MLOps\")  \n    (\"Tableau\" OR \"Power BI\" OR \"data visualization\")  \n    (\"statistical analysis\" OR \"A/B testing\")  \n    (\"NLP\" OR \"LLM\" OR \"GenAI\" OR \"recommendation systems\")  \n  )  \n  NOT (\"intern\" OR \"student\")  \n  Years: \"3+ years\"  \n  Location: \"Remote\" OR \"Mumbai\" OR \"Bangalore\" OR \"Pune\"  \n\"\"\")\nprint([doc.metadata[\"id\"] for doc in compressed_docs])\n\n# here eroor because reranl model in inbuilt you dont need to speccify","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:53:05.090294Z","iopub.execute_input":"2025-05-20T13:53:05.090578Z","iopub.status.idle":"2025-05-20T13:53:07.780636Z","shell.execute_reply.started":"2025-05-20T13:53:05.090557Z","shell.execute_reply":"2025-05-20T13:53:07.779535Z"}},"outputs":[{"name":"stderr","text":"ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:00<00:00, 272MiB/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1498498478.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Step 2: Initialize Flashrank with a reranker model (NOT gte-small)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mranker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRanker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ms-marco-MultiBERT-L-12\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Default model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcompressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlashrankRerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mranker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Rerank to top 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# compressor = FlashrankRerank(top_n= 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/document_compressors/flashrank_rerank.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_MODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"client\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRanker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flashrank/Ranker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, cache_dir, max_length, log_level)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_model_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_file_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'PosixPath' and 'Ranker'"],"ename":"TypeError","evalue":"unsupported operand type(s) for /: 'PosixPath' and 'Ranker'","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.document_compressors import FlashrankRerank\n\n# Initialize your base retriever (using gte-small embeddings)\nretriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever(search_kwargs={\"k\": 30})\n\n# Initialize FlashrankRerank - let it handle the Ranker internally\ncompressor = FlashrankRerank(top_n=5)  # This will automatically use ms-marco-MultiBERT-L-12\n\n# Create compression retriever\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)\n\n# Run your query\nquery = \"Machine Learning Engineer with Python and 3+ years experience in Bangalore\"\ncompressed_docs = compression_retriever.invoke(query)\n\n# Print results\nprint([doc.metadata.get(\"id\") for doc in compressed_docs])\n#  here errpor due tomssing code below\n# import langchain_community.document_compressors.flashrank_rerank as flashrank_rerank\n# flashrank_rerank.RerankRequest = RerankRequest\n# which s not mentned in lancan exmple","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# all corrected rearank code\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.document_compressors import FlashrankRerank\nfrom flashrank import Ranker, RerankRequest  # Explicitly import RerankRequest\n\n# Monkey patch the missing reference\nimport langchain_community.document_compressors.flashrank_rerank as flashrank_rerank\nflashrank_rerank.RerankRequest = RerankRequest\n\n# Initialize retriever\nretriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever(search_kwargs={\"k\": 30})\n\n# Initialize compressor\ncompressor = FlashrankRerank(top_n=5)\n\n# Create compression retriever\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)\n\n# Run query\nquery =  \"\"\"\n  \"Machine Learning Engineer\" OR \"Data Scientist\" AND (\n    (\"Python\" AND (\"Scikit-learn\" OR \"TensorFlow\" OR \"PyTorch\"))  \n    (\"SQL\" AND (\"Spark\" OR \"Hadoop\" OR \"ETL\"))  \n    (\"AWS\" OR \"GCP\" OR \"Azure\" OR \"MLOps\")  \n    (\"Tableau\" OR \"Power BI\" OR \"data visualization\")  \n    (\"statistical analysis\" OR \"A/B testing\")  \n    (\"NLP\" OR \"LLM\" OR \"GenAI\" OR \"recommendation systems\")  \n  )  \n  NOT (\"intern\" OR \"student\")  \n  Years: \"3+ years\"  \n  Location: \"Remote\" OR \"Mumbai\" OR \"Bangalore\" OR \"Pune\"  \n\"\"\"\ncompressed_docs = compression_retriever.invoke(query)\n\n# Print results\nprint([doc.metadata.get(\"id\") for doc in compressed_docs])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:59:32.177735Z","iopub.execute_input":"2025-05-20T13:59:32.178415Z","iopub.status.idle":"2025-05-20T13:59:53.055155Z","shell.execute_reply.started":"2025-05-20T13:59:32.178393Z","shell.execute_reply":"2025-05-20T13:59:53.054313Z"}},"outputs":[{"name":"stderr","text":"2025-05-20 13:59:37.828659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747749577.850671     229 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747749577.857466     229 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9c46f7c5c645f28d4544f85a6f19c5"}},"metadata":{}},{"name":"stdout","text":"[2, 15, 8, 6, 5]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"pprint.pp(compressed_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:59:55.732131Z","iopub.execute_input":"2025-05-20T13:59:55.732480Z","iopub.status.idle":"2025-05-20T13:59:55.737106Z","shell.execute_reply.started":"2025-05-20T13:59:55.732440Z","shell.execute_reply":"2025-05-20T13:59:55.736465Z"}},"outputs":[{"name":"stdout","text":"[Document(metadata={'id': 2, 'relevance_score': 0.9871056, 'Category': 'ENGINEERING', 'start_index': 0}, page_content='ENGINEERING INTERN\\nSkills\\nC++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters.\\nMachine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost\\nmodel that has 77.5% accuracy in the Kaggle Titanic challenge.)\\nComputational Fluid Dynamics and Discrete Element Method Codes\\nCFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS.\\nReservoir and Fracture Modeling Tools\\nCMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis.\\nExperimental and Statistical Methods\\nSEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\\nExperience\\nENGINEERING INTERN\\n \\n08/2016\\n \\nï¼\\u200b \\n12/2016\\n \\nCompany Name\\n \\nState\\nProject: Develop a cavings transport model for optimizing hole-cleaning operations.'),\n Document(metadata={'id': 15, 'relevance_score': 0.95992935, 'Category': 'AUTOMOBILE', 'start_index': 0}, page_content='Highlights\\nProg. Languages: \\nC (5+ yrs), Python (3+ yrs), Java (3+ yrs), MATLAB (Simulink) (5+ yrs), R (2 yrs), Processing (2yrs), SQL(4+ yrs),\\nPLC(2 yrs)\\nDoc. Editing: \\nWord/PPT/Excel, Pages/Numbers/Keynote, LATEX\\nMechanical Design: \\nAutoCAD (6 yrs), Solidworks (5+ yrs)\\nMechanical Skills: \\nMakerBot 3D print, Laser cut, Mill, Drill, Lathe Machine.\\nStatistics Softwares: \\nSTATA, SPSS\\nDatabase Softwares: \\nSQL Server (4 yrs), Navicat (2 yrs)\\nOperating Systems: \\nWindows 7/10, OS X\\nExperience\\nCompany Name\\n \\nJune 2016\\n \\nto \\nCurrent\\n \\nR&D Product Development Engineer\\n \\nCity\\nDesign and build a tail-sitter VTOL(vertical take off and landing) UAV(unmanned aerial vehicle) which.\\ntakes off and lands vertically and travels horizontally.\\nMain duties include but not limit to aerodynamics.\\nmodeling, UAV control system design, mechanical manufacturing, simulation and tuning/experiments.\\nCompany Name\\n \\nMay 2015\\n \\nto \\nFebruary 2016\\n \\nResearch assistant\\n \\nCity'),\n Document(metadata={'id': 8, 'relevance_score': 0.91915107, 'Category': 'BANKING', 'start_index': 819}, page_content='Pilot Run and User Acceptance testing.\\nApplication training, Go Live, Project sign-off.\\nWork with end-users to define and execute test scenarios and ensure appropriate end user training.\\nTechnical Responsibilities: Provide detailed system requirement to client(Hardware/Software).\\nProduction Server Setup (Windows 2003 Server / RedHat Linux 2.5).\\nDatabase setup (Oracle 10g R2.\\nIBM DB2,MS SQL 2005).\\nStandard database restore, Master Data preparation.\\nApplication server installation and configuration(Jboss 5.1.0.GA & Tomcat6).\\nSoftware deployments(ear,war etc.).\\nMaintaining Versions and Deliverable.\\nEducation\\nBachelor of Engineering\\n \\n, \\nInformation Technology\\n \\n6 2008\\n \\nUniversity of Mumbai\\n \\nIntegrated Trading and Manufacturing (ITM,An ERP by Base\\nInformation) BI Tool \\n: BI Base (Business Intelligence tool by Base Information) Information Technology\\nPersonal Information\\nComprehensive problem solving abilities, excellent verbal\\nInterests\\nPassport, Visa Details'),\n Document(metadata={'id': 6, 'relevance_score': 0.89844215, 'Category': 'INFORMATION-TECHNOLOGY', 'start_index': 0}, page_content='INFORMATION TECHNOLOGY INTERN (TEST AUTOMATION ENGINEER)\\nSummary\\nOver 3 yearsÂ of experience serving as a key contributor across all software development life cycleÂ phases includingÂ analysis,\\narchitectural design, prototyping, development, and testing of applicationÂ using Java/J2EE technologies in various domains.\\nVery good understanding of Object Oriented Programming, Data Structure, Algorithms, Design Patterns and Distributed Systems.\\nExcellent working experience in backendÂ development using different Spring modules like Spring Core ContainerÂ Module, AOP, MVC,\\nSecurity, Data, Transaction Management etc.\\nExperienced in developing Microservices with Spring Boot, Spring REST, Spring Cloud, etc.\\nExtensive experience in developing Web interfaces using HTML5, CSS3, Bootstrap, SASS, LESS, JavaScript, jQuery, AngularJS,\\nReactJS and BackboneJS.\\nExperienced in working with SQL databases like MySQL, PostgreSQL, Oracle and have some knowledge of NoSQL databases like\\nMongoDB.'),\n Document(metadata={'id': 5, 'relevance_score': 0.8976784, 'Category': 'AUTOMOBILE', 'start_index': 0}, page_content='DATA ANALYST\\nProfessional Summary\\nIndustrial and Systems Engineering graduate, certified Base SAS Programmer and a Lean Six Sigma Green Belt with strong background in\\nstatistics, mathematics and logical problem solving looking for a dynamic opportunity in data driven fields of analytics and statistical modeling.\\nCore Qualifications\\nData Science Tools: R, Base SAS, Python (Numpy, Pandas, Matplotlib, Scikit- learn), SPSS, Minitab, MATLAB, Apache Spark, SQL, MS\\nExcel, MS Visio, Tableau MySQL, Oracle Database, Microsoft Access Key Competencies: Data Extraction, Data Wrangling, Data Analysis,\\nData Visualization, Regression Analysis (Linear, Logistic and Multinomial), Time Series Analysis, Association Rule Mining, Monte Carlo\\nSimulation, Optimization, Random Forests\\nExperience\\n07/2016\\n \\nto \\nCurrent\\nData Analyst\\n \\nCompany Name\\n \\nï¼\\u200b \\nState\\n09/2015\\n \\nto \\n05/2016\\nStudent Manager\\n \\nCompany Name\\n \\nï¼\\u200b \\nState')]\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}